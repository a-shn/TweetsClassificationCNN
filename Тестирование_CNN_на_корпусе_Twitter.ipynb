{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Тестирование CNN на корпусе Twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z8c5lvxoEkCR",
        "mFhZfaVbnct3",
        "DwUFo8wz6nvV"
      ],
      "toc_visible": true,
      "mount_file_id": "1HggcJqGs1W63uG7O6PYCiFGp7QpwA5c9",
      "authorship_tag": "ABX9TyNbzrhNvJrQSkXjQPtBarkg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-shn/TweetsClassificationCNN/blob/master/%D0%A2%D0%B5%D1%81%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_CNN_%D0%BD%D0%B0_%D0%BA%D0%BE%D1%80%D0%BF%D1%83%D1%81%D0%B5_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMyshtehDqc2"
      },
      "source": [
        "# **Результаты в виде таблицы:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf4TPld8ApeS"
      },
      "source": [
        "upd. С нормализацией, с лемматизацией:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKdpnTG4ApFq"
      },
      "source": [
        "Train corpus | Test corpus | Precision | Recall | F-measure\n",
        "--- | --- | --- | --- | ---\n",
        "train | validation | 0.27869 | 0.76692 | 0.40882\n",
        "train + validation | test | 0.27296 | 0.64458 | 0.38351\n",
        "train + validation + test | test_final | 0.32890 | 0.87374 | 0.47790"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ0cxQOhKYiw"
      },
      "source": [
        "С нормализацией, без лемматизации:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_3FjymrnfKY"
      },
      "source": [
        "Train corpus | Test corpus | Precision | Recall | F-measure\n",
        "--- | --- | --- | --- | ---\n",
        "train | validation | 0.27559 | 0.78947 | 0.40856\n",
        "train + validation | test | 0.29630 | 0.67470 | 0.41176\n",
        "train + validation + test | test_final | 0.36881 | 0.75253 | 0.49502"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sile3uzWKiOv"
      },
      "source": [
        "Без нормализации, без лемматизации:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJX3ZswmKhY8"
      },
      "source": [
        "Train corpus | Test corpus | Precision | Recall | F-measure\n",
        "--- | --- | --- | --- | ---\n",
        "train | validation | 0.64705 | 0.24812 | 0.35869\n",
        "train + validation | test | 0.51063 | 0.28915 | 0.36923\n",
        "train + validation + test | test_final | 0.74698 | 0.31313 | 0.44128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZOKsx1kK4BG"
      },
      "source": [
        "Без нормализации, с лемматизацией:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdXQvsymK3g7"
      },
      "source": [
        "Train corpus | Test corpus | Precision | Recall | F-measure\n",
        "--- | --- | --- | --- | ---\n",
        "train | validation | 0.60000 | 0.27068 | 0.37306\n",
        "train + validation | test | 0.54000 | 0.16265 | 0.25000\n",
        "train + validation + test | test_final | 0.72340 | 0.34343 | 0.46575"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TJdoniVFJKn"
      },
      "source": [
        "**Over/under-sampling results:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcdz0a3Oroc4"
      },
      "source": [
        "Method | Precision | Recall | F-measure\n",
        "--- | --- | --- | ---\n",
        "RandomUnderSampler | 0.27873 | 0.68675 | 0.39652\n",
        "NearMiss (ver.1) | 0.13662 | 0.88554 | 0.23671\n",
        "NearMiss (ver.2) | 0.21651 | 0.83735 | 0.34406\n",
        "NearMiss (ver.3) | 0.22078 | 0.81928 | 0.34783\n",
        "EditedNearestNeighbours | 0.25747 | 0.67470 | 0.37271\n",
        "RandomOverSampler | 0.46087 | 0.31928 | 0.37722\n",
        "SMOTE | 0.28295 | 0.43976 | 0.34434\n",
        "ADASYN | 0.28163 | 0.41566 | 0.33577\n",
        "SMOTEENN | 0.22545 | 0.60843 | 0.32899\n",
        "SMOTETomek | 0.27007 | 0.44578 | 0.33636"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLxJMoAOCYBQ"
      },
      "source": [
        "Лучшие результаты у RandomUnderSampler, результаты для train+validation+test и оценкой на test_final:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRzElO3eCxS7"
      },
      "source": [
        "Train corpus | Test corpus | Precision | Recall | F-measure\n",
        "--- | --- | --- | --- | ---\n",
        "train + validation + test | test_final | 0.37740 | 0.79293 | 0.51140"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ3QqaHMzlHg"
      },
      "source": [
        "**MUSE vectors:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll-Ew8BmzsAe"
      },
      "source": [
        "Train corpus | Test corpus | Precision | Recall | F-measure\n",
        "--- | --- | --- | --- | ---\n",
        "train + validation + test | test_final | 0.29205 | 0.79798 | 0.42760"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30gaggnm1tnc"
      },
      "source": [
        "**LASER vectors:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDuH55cS1xFz"
      },
      "source": [
        "Train corpus | Test corpus | Precision | Recall | F-measure\n",
        "--- | --- | --- | --- | ---\n",
        "train + validation + test | test_final | 0.32376 | 0.62626 | 0.42685"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvCOZwilFAKl"
      },
      "source": [
        "# **Считывание и разделение данных, инициализация TextCNN и импортирование fasttext-модели**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSRK13jNKGM5"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXSJuA-p_HWC"
      },
      "source": [
        "KEEP_COLUMNS = [\"class\", \"tweet\"]\n",
        "\n",
        "training_raw_path = '/content/drive/My Drive/NLP/RuADReCT_raw/task2_ru_training_raw.tsv'\n",
        "valiadtion_raw_path = '/content/drive/My Drive/NLP/RuADReCT_raw/task2_ru_validation_raw.tsv'\n",
        "test_raw_path = '/content/drive/My Drive/NLP/RuADReCT_raw/task2_ru_test_raw.tsv'\n",
        "test_final_raw_path = '/content/drive/My Drive/NLP/RuADReCT_raw/task2_ru_test_final_raw.tsv'\n",
        "\n",
        "fasttext_model_path = '/content/drive/My Drive/NLP/rudrec_fasttext/rudrec_fasttext_model.bin'\n",
        "\n",
        "muse_supervised_path = '/content/drive/My Drive/NLP/MUSE/wiki.multi.ru.vec'\n",
        "muse_bin_path = '/content/wiki.multi.ru.bin'\n",
        "\n",
        "laser_input_words = '/content/drive/My Drive/NLP/LASER/input_text.txt'\n",
        "laser_embeddings = '/content/drive/My Drive/NLP/LASER/tweet_embeddings.raw'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCJRaq_BR41-"
      },
      "source": [
        "# В зависимости от задания, откомментировать нужную часть\n",
        "\n",
        "# 1 Задание:\n",
        "# train_df = pd.read_csv(training_raw_path, sep=\"\\t\", encoding=\"utf-8\")\n",
        "# test_df = pd.read_csv(valiadtion_raw_path, sep=\"\\t\", encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "# 2 Задание:\n",
        "# train_df = pd.concat([pd.read_csv(training_raw_path, sep=\"\\t\", encoding=\"utf-8\"), \n",
        "#                       pd.read_csv(valiadtion_raw_path, sep=\"\\t\", encoding=\"utf-8\")])\n",
        "# test_df = pd.read_csv(test_raw_path, sep=\"\\t\", encoding=\"utf-8\")\n",
        "\n",
        "# 3 Задание:\n",
        "train_df = pd.concat([pd.read_csv(training_raw_path, sep=\"\\t\", encoding=\"utf-8\"), \n",
        "                      pd.read_csv(valiadtion_raw_path, sep=\"\\t\", encoding=\"utf-8\"),\n",
        "                      pd.read_csv(test_raw_path, sep=\"\\t\", encoding=\"utf-8\")])\n",
        "test_df = pd.read_csv(test_final_raw_path, sep=\"\\t\", encoding=\"utf-8\")\n",
        "\n",
        "train_df = train_df[KEEP_COLUMNS] \n",
        "train_df = train_df[(train_df['class'] == 0) | (train_df['class'] == 1)] # Удаляем невалидные строки (класс NaN, например)\n",
        "test_df = test_df[KEEP_COLUMNS]\n",
        "test_df = test_df[(test_df['class'] == 0) | (test_df['class'] == 1)]\n",
        "\n",
        "#P.S. в task2_ru_test_final_raw.tsv название колонки с твитами называется \"text\", а не \"tweet\", \n",
        "#как в других, поэтому перед запуском 3-его задания, нужно ее переименовать в \"tweet\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FkFCzId_r6x"
      },
      "source": [
        "train_df, dev_df, _, _ = \\\n",
        "    train_test_split(train_df, train_df, test_size=0.1, random_state=42)\n",
        "train_positive_class_df = train_df[train_df['class'] == 1]\n",
        "train_negative_class_df = train_df[train_df['class'] == 0]\n",
        "num_min_examples = min(train_positive_class_df.shape[0], train_negative_class_df.shape[0])\n",
        "train_positive_class_df = train_positive_class_df.sample(num_min_examples)\n",
        "train_negative_class_df = train_negative_class_df.sample(num_min_examples)\n",
        "\n",
        "class_normalized_train_df = pd.concat([train_positive_class_df, train_negative_class_df]).sample(frac=1)\n",
        "# train_df = class_normalized_train_df # для обучения без нормирования - закомментировать"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTJKQq2hMPtV"
      },
      "source": [
        "!pip install gensim \n",
        "!pip install tensorflow\n",
        "!pip install fasttext\n",
        "!pip install pymorphy2\n",
        "!pip install imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YEg5S1tMV4I"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate\n",
        "\n",
        "class TextCNN(Model):\n",
        "    def __init__(self,\n",
        "                 maxlen,\n",
        "                 max_features,\n",
        "                 embedding_dims,\n",
        "                 kernel_sizes=[3, 4, 5],\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid',\n",
        "                 embedding_weights=None):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        self.embedding = Embedding(self.max_features, self.embedding_dims,\n",
        "                                   input_length=self.maxlen, weights=[embedding_weights], )\n",
        "        self.convs = []\n",
        "        self.max_poolings = []\n",
        "        for kernel_size in self.kernel_sizes:\n",
        "            self.convs.append(Conv1D(128, kernel_size, activation='relu'))\n",
        "            self.max_poolings.append(GlobalMaxPooling1D())\n",
        "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.get_shape()) != 2:\n",
        "            raise ValueError('The rank of inputs of TextCNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
        "        if inputs.get_shape()[1] != self.maxlen:\n",
        "            raise ValueError(\n",
        "                'The maxlen of inputs of TextCNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
        "        # Embedding part can try multichannel as same as origin paper\n",
        "        embedding = self.embedding(inputs)\n",
        "        convs = []\n",
        "        for i in range(len(self.kernel_sizes)):\n",
        "            c = self.convs[i](embedding)\n",
        "            c = self.max_poolings[i](c)\n",
        "            convs.append(c)\n",
        "        x = Concatenate()(convs)\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F9ImOWQNG1u"
      },
      "source": [
        "from collections import Counter\n",
        "from gensim.models import KeyedVectors\n",
        "import fasttext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymorphy2\n",
        "from keras_preprocessing import sequence\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diLag1ylQicz"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EMBEDDINGS_DIM = 1024\n",
        "CLASSIFIER_TRAIN_EPOCHS = 10\n",
        "CLASSIFICATION_THRESHOLD = 0.5"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqssIqjt2VLi"
      },
      "source": [
        "**FastText/MUSE:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhcvRtpMQxVA"
      },
      "source": [
        "# Loading pretrained fastext model\n",
        "# fasttext_model = fasttext.load_model(fasttext_model_path)\n",
        "muse_model = KeyedVectors.load_word2vec_format(muse_supervised_path, binary=False) # MUSE\n",
        "\n",
        "# Extracting tweet texts\n",
        "train_tweet_texts = train_df.tweet.values\n",
        "test_tweet_texts = test_df.tweet.values\n",
        "dev_tweet_texts = dev_df.tweet.values\n",
        "\n",
        "# Extracting tweet labels\n",
        "train_labels = train_df['class'].values\n",
        "test_labels = test_df['class'].values\n",
        "dev_labels = dev_df['class'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCLRvtwfnf8s"
      },
      "source": [
        "**LASER:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm5n5tD1nj4h"
      },
      "source": [
        "from numpy import loadtxt\n",
        "\n",
        "words = loadtxt(laser_input_words, dtype=\"str\", delimiter=\",\", unpack=False)\n",
        "dim = 1024\n",
        "laser_vectors = dict()\n",
        "embeddings = np.fromfile(laser_embeddings, dtype=np.float32, count=-1)\n",
        "embeddings.resize(embeddings.shape[0] // dim, dim)\n",
        "for word, vector in zip(words, embeddings):\n",
        "  laser_vectors[word] = vector\n",
        "\n",
        "# Extracting tweet texts\n",
        "train_tweet_texts = train_df.tweet.values\n",
        "test_tweet_texts = test_df.tweet.values\n",
        "dev_tweet_texts = dev_df.tweet.values\n",
        "\n",
        "# Extracting tweet labels\n",
        "train_labels = train_df['class'].values\n",
        "test_labels = test_df['class'].values\n",
        "dev_labels = dev_df['class'].values"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4tEnyVy4UQL",
        "outputId": "06b56dfd-5b73-4ebc-fa4d-95c88dff4333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(laser_vectors.get(\"ксанакс\"))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8c5lvxoEkCR"
      },
      "source": [
        "# **Пре-процессинг:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhDUe5W7MDoN"
      },
      "source": [
        "import re\n",
        "def list_replace(search, replacement, text):\n",
        "    \"\"\"\n",
        "    Replaces all symbols of text which are present\n",
        "    in the search string with the replacement string.\n",
        "    \"\"\"\n",
        "    search = [el for el in search if el in text]\n",
        "    for c in search:\n",
        "        text = text.replace(c, replacement)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = list_replace \\\n",
        "        ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
        "\n",
        "    text = list_replace \\\n",
        "        ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
        "\n",
        "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
        "\n",
        "    text = list_replace \\\n",
        "            (\n",
        "            '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
        "            '\\u2002', text)\n",
        "\n",
        "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
        "    text = re.sub('\\t\\t', '\\t', text)\n",
        "\n",
        "    text = list_replace \\\n",
        "            (\n",
        "            '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
        "            '.', text)\n",
        "\n",
        "    text = list_replace('\\u2217', '\\u002A', text)\n",
        "\n",
        "    text = list_replace('…', '...', text)\n",
        "\n",
        "    text = list_replace('\\u00C4', 'A', text)\n",
        "    text = list_replace('\\u00E4', 'a', text)\n",
        "    text = list_replace('\\u00CB', 'E', text)\n",
        "    text = list_replace('\\u00EB', 'e', text)\n",
        "    text = list_replace('\\u1E26', 'H', text)\n",
        "    text = list_replace('\\u1E27', 'h', text)\n",
        "    text = list_replace('\\u00CF', 'I', text)\n",
        "    text = list_replace('\\u00EF', 'i', text)\n",
        "    text = list_replace('\\u00D6', 'O', text)\n",
        "    text = list_replace('\\u00F6', 'o', text)\n",
        "    text = list_replace('\\u00DC', 'U', text)\n",
        "    text = list_replace('\\u00FC', 'u', text)\n",
        "    text = list_replace('\\u0178', 'Y', text)\n",
        "    text = list_replace('\\u00FF', 'y', text)\n",
        "    text = list_replace('\\u00DF', 's', text)\n",
        "    text = list_replace('\\u1E9E', 'S', text)\n",
        "    # Removing punctuation\n",
        "    text = list_replace(',.[]{}()=+-−*&^%$#@!~;:§/\\|\\?\"\\n', ' ', text)\n",
        "    # Replacing all numbers with masks\n",
        "    text = list_replace('0123456789', 'x', text)\n",
        "\n",
        "    currencies = list \\\n",
        "            (\n",
        "            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
        "        )\n",
        "\n",
        "    alphabet = list \\\n",
        "            (\n",
        "            '\\t\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
        "\n",
        "    allowed = set(currencies + alphabet)\n",
        "\n",
        "    cleaned_text = [sym for sym in text if sym in allowed]\n",
        "    cleaned_text = ''.join(cleaned_text)\n",
        "\n",
        "    return cleaned_text"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxmA_QYARlko"
      },
      "source": [
        "**Очистка:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSKRJDV7N74T"
      },
      "source": [
        "maxlen = 0\n",
        "# Preprocessing training tweets\n",
        "cleaned_train_texts = []\n",
        "for tweet_text in train_tweet_texts:\n",
        "    cleaned_text = clean_text(tweet_text).lower()\n",
        "    split_cleaned_text = cleaned_text.split()\n",
        "    # Estimating max length of all training tweets in tokens\n",
        "    if len(split_cleaned_text) > maxlen:\n",
        "        maxlen = len(split_cleaned_text)\n",
        "    cleaned_train_texts.append(\" \".join(split_cleaned_text))\n",
        "    \n",
        "# Preprocessing test tweets\n",
        "cleaned_test_texts = []\n",
        "for tweet_text in test_tweet_texts:\n",
        "    cleaned_text = clean_text(tweet_text)\n",
        "    cleaned_test_texts.append(\" \".join(cleaned_text.split()))\n",
        "    \n",
        "# Preprocessing validation tweets\n",
        "cleaned_dev_texts = []\n",
        "for tweet_text in dev_tweet_texts:\n",
        "    cleaned_text = clean_text(tweet_text)\n",
        "    cleaned_dev_texts.append(\" \".join(cleaned_text.split()))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmnq_eSyRxGh"
      },
      "source": [
        "**Лемматизация:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSbEOz8ZS7eF"
      },
      "source": [
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZbmwxkYR3NC"
      },
      "source": [
        "def lemm(texts):\n",
        "    texts = [ww.split(' ') for ww in texts]\n",
        "    texts = list(map(lambda ww: [morph.parse(w)[0].normal_form for w in ww], texts))\n",
        "    texts = [' '.join(ww) for ww in texts]\n",
        "    return texts"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9mJocD3SMEt"
      },
      "source": [
        "cleaned_train_texts = lemm(cleaned_train_texts)\n",
        "cleaned_test_texts = lemm(cleaned_test_texts)\n",
        "cleaned_dev_texts = lemm(cleaned_dev_texts)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTY_T9xDGsg7"
      },
      "source": [
        "# **Эмбеддинг:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Zpi8L-OPDk"
      },
      "source": [
        "tokenizer = Tokenizer(lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(cleaned_train_texts + cleaned_test_texts + cleaned_dev_texts)\n",
        "# Converting texts to lists of ids\n",
        "word_seq_train = tokenizer.texts_to_sequences(cleaned_train_texts)\n",
        "word_seq_test = tokenizer.texts_to_sequences(cleaned_test_texts)\n",
        "word_seq_dev = tokenizer.texts_to_sequences(cleaned_dev_texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Padding too short tweet texts with '0's\n",
        "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=maxlen)\n",
        "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=maxlen)\n",
        "word_seq_dev = sequence.pad_sequences(word_seq_dev, maxlen=maxlen)\n",
        "\n",
        "dictionary_size = len(word_index.keys())\n",
        "# 0-th token of embedding matrix is a padding token\n",
        "embedding_matrix = np.zeros((dictionary_size + 1, EMBEDDINGS_DIM))\\\n",
        "\n",
        "vector_count = 0\n",
        "for word, i in word_index.items(): # LASER\n",
        "  try:\n",
        "    embedding_vector = laser_vectors.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        vector_count += 1\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "# for word, i in word_index.items(): # MUSE\n",
        "#   try:\n",
        "#     embedding_vector = muse_model.get_vector(word)\n",
        "#     if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         vector_count += 1\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#   except:\n",
        "#     continue\n",
        "\n",
        "# for word, i in word_index.items(): # FASTTEXT\n",
        "#     embedding_vector = fasttext_model.get_word_vector(word)\n",
        "#     if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         vector_count += 1\n",
        "#         embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jADYYHEZyQJa"
      },
      "source": [
        "# **Under-sampling section:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VsbTm84yYpP"
      },
      "source": [
        "**RandomUnderSampler:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXp41cRByXL8",
        "outputId": "a421255d-6a93-4dc7-e165-e1a746d87bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "rus = RandomUnderSampler(random_state=2)\n",
        "word_seq_train_res, train_labels_res = rus.fit_resample(word_seq_train, train_labels)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoGOjR9IypAS",
        "outputId": "7ee929bd-0969-4ccb-f907-21a24c6e3404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(Counter(train_labels))\n",
        "print(Counter(train_labels_res))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0.0: 7812, 1.0: 750})\n",
            "Counter({0.0: 750, 1.0: 750})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNtvtJyCzOWV"
      },
      "source": [
        "**Near-miss:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pobe8q16zVrS"
      },
      "source": [
        "from imblearn.under_sampling import NearMiss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUFXlrchzfo_"
      },
      "source": [
        "nm = NearMiss(version=1, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT5fesqV0cUp"
      },
      "source": [
        "nm = NearMiss(version=2, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osbzY4ny0byK"
      },
      "source": [
        "nm = NearMiss(version=3, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcI9uotk0YoS",
        "outputId": "9c954dd2-5656-4539-cba2-fbdeec1ec46d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_seq_train_res, train_labels_res = nm.fit_resample(word_seq_train, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XArrZ-Ee1cC1"
      },
      "source": [
        "**EditedNearestNeighbours:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2yQQakF1fgf",
        "outputId": "64bbd43d-ce4e-4a44-9e35-c458be694da9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "enn = EditedNearestNeighbours(random_state=2)\n",
        "word_seq_train_res, train_labels_res = rus.fit_resample(word_seq_train, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFhZfaVbnct3"
      },
      "source": [
        "# **Over-sampling section:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_OCyzF5tpts"
      },
      "source": [
        "**RandomOverSampler:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss5b_dvPttHm"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=2)\n",
        "word_seq_train_res, train_labels_res = ros.fit_resample(word_seq_train, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQjTPHDCuUIu",
        "outputId": "7b360d1e-b9ee-44cd-9e76-862facae6d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Counter(train_labels))\n",
        "print(Counter(train_labels_res))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0.0: 6256, 1.0: 593})\n",
            "Counter({0.0: 593, 1.0: 593})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuu_NRCjoAKk"
      },
      "source": [
        "**SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmSe2o2Yn1JL"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=2)\n",
        "word_seq_train_res, train_labels_res = sm.fit_resample(word_seq_train, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3mu_NT4pWai"
      },
      "source": [
        "print(Counter(train_labels))\n",
        "print(Counter(train_labels_res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB3HsJNZwAQn"
      },
      "source": [
        "**ADASYN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJRp-4A3wERD",
        "outputId": "07598225-d35c-4d82-c855-159ce56a7589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "adsn = ADASYN(random_state=2)\n",
        "word_seq_train_res, train_labels_res = adsn.fit_resample(word_seq_train, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR1RSd3JwZgR",
        "outputId": "96314a57-586f-46d5-e16e-0900058965a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Counter(train_labels))\n",
        "print(Counter(train_labels_res))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0.0: 6256, 1.0: 593})\n",
            "Counter({0.0: 6226, 1.0: 6226})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwUFo8wz6nvV"
      },
      "source": [
        "# **Combine-sampling section:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si-BBWtk6yGH"
      },
      "source": [
        "**SMOTEENN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xND6lih61s9",
        "outputId": "fdf2ddfc-12ca-4566-c8a1-5807d1f55355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "smtnn = SMOTEENN(random_state=2)\n",
        "word_seq_train_res, train_labels_res = smtnn.fit_resample(word_seq_train, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXJfppDN8z69"
      },
      "source": [
        "**SMOTETomek**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elGzebH-82T-",
        "outputId": "26bbd7e2-3111-4641-8215-9ff19bdfd5b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from imblearn.combine import SMOTETomek\n",
        "smttmk = SMOTETomek(random_state=2)\n",
        "word_seq_train_res, train_labels_res = smttmk.fit_resample(word_seq_train, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAU69TK-GXQx"
      },
      "source": [
        "# **Компиляция модели:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvh_42byPBDP"
      },
      "source": [
        "model = TextCNN(maxlen, dictionary_size + 1, EMBEDDINGS_DIM, embedding_weights=embedding_matrix)\n",
        "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'], )\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3 , mode='max', restore_best_weights=True)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI8dUcTuGHv6"
      },
      "source": [
        "# **Обучение модели:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCxmIJa3PV-m",
        "outputId": "c1d3230a-0983-4b53-8066-025da3aa3545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "model.fit(word_seq_train_res, train_labels_res,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=CLASSIFIER_TRAIN_EPOCHS,\n",
        "              callbacks=[early_stopping, ],\n",
        "              validation_data=(word_seq_dev, dev_labels))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "12/12 [==============================] - 26s 2s/step - loss: 0.6670 - accuracy: 0.6027 - val_loss: 0.5647 - val_accuracy: 0.8372\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 25s 2s/step - loss: 0.4540 - accuracy: 0.8727 - val_loss: 0.4473 - val_accuracy: 0.8078\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 26s 2s/step - loss: 0.1854 - accuracy: 0.9453 - val_loss: 0.4540 - val_accuracy: 0.8025\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 25s 2s/step - loss: 0.0429 - accuracy: 0.9927 - val_loss: 0.9082 - val_accuracy: 0.6649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe199a8f6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmZYpcSxG_0p"
      },
      "source": [
        "# **Вывод результатов:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M1CDE0yZFTk"
      },
      "source": [
        "predicted_test_prob = model.predict(word_seq_test)\n",
        "predicted_test_labels = []\n",
        "predicted_dev_prob = model.predict(word_seq_dev)\n",
        "predicted_dev_labels = []\n",
        "\n",
        "for subarray in predicted_test_prob:\n",
        "    label = 1 if subarray[0] >= CLASSIFICATION_THRESHOLD else 0\n",
        "    predicted_test_labels.append(label)\n",
        "\n",
        "for subarray in predicted_dev_prob:\n",
        "    label = 1 if subarray[0] >= CLASSIFICATION_THRESHOLD else 0\n",
        "    predicted_dev_labels.append(label)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7I169ljZc_N",
        "outputId": "17e44be1-800c-49da-8043-bf218dfd328e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "dev_precision = precision_score(dev_labels, predicted_dev_labels, )\n",
        "dev_recall = recall_score(dev_labels, predicted_dev_labels, )\n",
        "dev_f_measure = f1_score(dev_labels, predicted_dev_labels, )\n",
        "print(f\"Dev:\\nPrecision: {dev_precision}\\n\"\n",
        "        f\"Recall: {dev_recall}\\nF-measure: {dev_f_measure}\")\n",
        "\n",
        "test_precision = precision_score(test_labels, predicted_test_labels, )\n",
        "test_recall = recall_score(test_labels, predicted_test_labels, )\n",
        "test_f_measure = f1_score(test_labels, predicted_test_labels, )\n",
        "print(f\"Test:\\nPrecision: {test_precision}\\n\"\n",
        "        f\"Recall: {test_recall}\\nF-measure: {test_f_measure}\\n\")\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dev:\n",
            "Precision: 0.281437125748503\n",
            "Recall: 0.573170731707317\n",
            "F-measure: 0.3775100401606426\n",
            "Test:\n",
            "Precision: 0.3237597911227154\n",
            "Recall: 0.6262626262626263\n",
            "F-measure: 0.4268502581755594\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3hCr3BoZx0-",
        "outputId": "0be7fbe6-2015-47b0-80ac-b8ffd14ced3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "print(classification_report(test_labels, predicted_test_labels, digits=5))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.93399   0.80168   0.86279      1306\n",
            "           1    0.32376   0.62626   0.42685       198\n",
            "\n",
            "    accuracy                        0.77859      1504\n",
            "   macro avg    0.62887   0.71397   0.64482      1504\n",
            "weighted avg    0.85365   0.77859   0.80540      1504\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troKDfAMDkAr"
      },
      "source": [
        "#**Cоздание таблицы с вероятностью принадлежности классу:**#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj1p-97sJ5_5"
      },
      "source": [
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa6a4oH8DjOr"
      },
      "source": [
        "with open('predict_table.csv', 'w', newline='') as csvfile:\n",
        "\t\tcsvfile.write('id,class,prob\\n')\n",
        "\t\twriter = csv.DictWriter(csvfile, fieldnames=['id', 'class', 'prob'])\n",
        "\t\tfor i in range(test_df.shape[0]):\n",
        "\t\t\t  writer.writerow({'id': test_df['id'][i], 'class': test_df['class'][i], 'prob': predicted_test_prob[i][0]})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}